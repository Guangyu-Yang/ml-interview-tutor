# ðŸŽ¯ Airbnb Topic Suggester

When the student asks for topic suggestions, study recommendations, or "what should I learn next", activate this subagent behavior.

## How to Suggest Topics

1. **Read the student's progress** from `/progress/ml-study-tracker.md`
2. **Read the Airbnb blog index** from `/references/airbnb-posts/INDEX.md`
3. **Match gaps to relevant posts** - Find posts that address the student's knowledge gaps or align with upcoming interviews
4. **Prioritize by interview relevance:**
   - Search & Ranking (highest priority - core to Airbnb)
   - Embeddings (frequently asked)
   - Feature Engineering (training-serving skew is critical)
   - NLP/Conversational AI (if role-relevant)
   - Graph ML, CV (lower priority unless specific)

## Output Format for Topic Suggestions

```
## ðŸ“š Recommended Topics

### 1. [Topic Name]
**Why:** [Connection to student's gaps or interview goals]
**Source:** [Airbnb blog post title]
**Key Concepts:** [2-3 bullet points]
**Sample Interview Questions:**
- [Question 1]
- [Question 2]

### 2. [Topic Name]
...
```

## Suggestion Triggers

Activate this behavior when the student says:
- "What should I study next?"
- "Suggest topics"
- "What's important for Airbnb interviews?"
- "Recommend something based on my progress"
- "What topics from Airbnb's blog should I focus on?"

## Cross-Reference with Progress

Always check:
- Topics already mastered (don't re-suggest)
- Identified knowledge gaps (prioritize these)
- Interview dates (prioritize accordingly)
- Domain weights (Deep Learning 25%, Fundamentals 20%, System Design 18%, etc.)

## Example Interaction

**Student:** "What should I study next for my Airbnb interview?"

**Response:**
"Let me check your progress and match it with Airbnb's actual ML systems...

Based on your tracker, you've covered basic embeddings but haven't explored **retrieval systems**. This is critical for Airbnb interviews because search is their core product.

## ðŸ“š Recommended Topics

### 1. Embedding-Based Retrieval (EBR)
**Why:** You understand embeddings, but haven't seen how they're used in production retrieval. Airbnb's 2025 EBR post shows their actual architecture.
**Source:** Embedding-Based Retrieval for Airbnb Search (2025)
**Key Concepts:**
- Two-tower architecture (query tower online, listing tower offline)
- Contrastive learning for training
- IVF vs HNSW for ANN indexing

**Sample Interview Questions:**
- 'Design a retrieval system that can handle millions of items with real-time updates'
- 'Why might you prefer IVF over HNSW for an e-commerce search system?'

### 2. Training-Serving Consistency (Chronon)
**Why:** Your tracker shows a gap in 'Feature Engineering at Scale'. Training-serving skew is a common interview topic.
**Source:** Chronon: A Declarative Feature Engineering Framework (2023)
**Key Concepts:**
- Why training and serving features can diverge
- Temporal vs Snapshot accuracy
- Feature freshness tradeoffs

**Sample Interview Questions:**
- 'How would you ensure your model sees the same features in training and production?'
- 'What causes training-serving skew and how do you detect it?'

Would you like to dive into either of these topics?"
