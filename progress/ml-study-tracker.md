# ML Interview Study Tracker

**Last Updated**: 2026-02-09

**Target Interview Date**: [Your interview date]

**Overall Interview Readiness**: 27%

---

## Quick Stats

| Domain | Topics Covered | Total Topics | Status |
|--------|---------------|--------------|--------|
| A. ML Fundamentals (20%) | 2 | 8 | üü° In Progress |
| B. Classical ML (15%) | 0 | 7 | üî¥ Not Started |
| C. Deep Learning (25%) | 4 (+1 in progress) | 8 | üü° In Progress |
| D. NLP (12%) | 0 | 6 | üî¥ Not Started |
| E. ML System Design (18%) | 4 | 8 | üü° In Progress |
| F. Practical ML (10%) | 0 | 6 | üî¥ Not Started |

**Status Legend**: üî¥ Not Started | üü° In Progress | üü¢ Interview Ready

---

## Study Priority (Based on Weights)

1. üî• **Deep Learning (25%)** - Transformers, attention, CNNs, RNNs ‚Äî GOOD PROGRESS
2. üî• **ML Fundamentals (20%)** - Gradient descent, bias-variance, regularization
3. üìå **ML System Design (18%)** - Pipelines, serving, A/B testing ‚Äî STARTED
4. üìå **Classical ML (15%)** - Trees, SVM, clustering
5. üìã **NLP (12%)** - Embeddings, BERT, transformers for NLP
6. üìã **Practical ML (10%)** - Debugging, imbalanced data

---

## Topics Mastered

### A. ML Fundamentals
| Topic | Date Mastered | Confidence | Key Points |
|-------|---------------|------------|------------|
| **A.8 Evaluation Metrics - AUC-ROC** | 2026-02-02 | Medium-High | ‚Ä¢ Measures ranking ability across all thresholds<br>‚Ä¢ AUC = P(random positive ranks higher than random negative)<br>‚Ä¢ Threshold-independent, handles imbalanced data<br>‚Ä¢ AUC=0.5 (random), AUC=1.0 (perfect)<br>‚Ä¢ Use AUC for flexible thresholds; Precision@k for fixed top-k<br>‚Ä¢ ROC axes: x=FPR, y=TPR |
| **A.3 Gradient Descent & Optimization - Logistic Regression** | 2026-02-02 | High | ‚Ä¢ Derived complete gradient from first principles<br>‚Ä¢ Chain rule: ‚àÇL/‚àÇw = (‚àÇL/‚àÇ≈∑)(‚àÇ≈∑/‚àÇz)(‚àÇz/‚àÇw)<br>‚Ä¢ Beautiful simplification: ‚àÇL/‚àÇz = ≈∑ - y<br>‚Ä¢ Complete gradient: ‚àÇL/‚àÇw = (≈∑ - y)x + Œªw<br>‚Ä¢ L2 regularization adds Œªw term (weight decay)<br>‚Ä¢ Can derive on whiteboard for interviews |

### B. Classical ML
| Topic | Date Mastered | Confidence | Key Points |
|-------|---------------|------------|------------|
| ‚Äî | ‚Äî | ‚Äî | ‚Äî |

### C. Deep Learning
| Topic | Date Mastered | Confidence | Key Points |
|-------|---------------|------------|------------|
| **C.21 Transformers & Self-Attention** | 2026-02-02 | High | ‚Ä¢ Self-attention solves RNN bottlenecks (parallel + direct connections)<br>‚Ä¢ O(n¬≤) complexity trade-off for long sequences<br>‚Ä¢ Q, K, V mechanism: similarity-weighted information retrieval<br>‚Ä¢ Positional encodings needed (RoPE, sinusoidal, learned)<br>‚Ä¢ BERT (encoder, bidirectional) vs GPT (decoder, causal)<br>‚Ä¢ Can explain architecture choices for different tasks |
| **C.21 Multi-Head Attention** | 2026-02-03 | High | ‚Ä¢ Different heads learn different relationship types (syntactic, semantic, positional)<br>‚Ä¢ Examples: subject-verb, pronoun resolution, induction heads<br>‚Ä¢ d_k = d_model / num_heads (e.g., 512/8 = 64)<br>‚Ä¢ W^O projection fuses knowledge across heads<br>‚Ä¢ More heads needed for complex sequences (more patterns to capture) |
| **C.16 Backpropagation** | 2026-02-03 | High | ‚Ä¢ Chain rule applied layer-by-layer: ‚àÇL/‚àÇW‚ÇÅ = (≈∑-y) √ó W‚ÇÇ √ó ReLU'(z‚ÇÅ) √ó x<br>‚Ä¢ Error signal (Œ¥) computed once per layer, reused for all gradients<br>‚Ä¢ ‚àÇL/‚àÇW = Œ¥ √ó (input to that layer)<br>‚Ä¢ Vanishing gradients: small weights multiply ‚Üí tiny gradients<br>‚Ä¢ Exploding gradients: large weights multiply ‚Üí huge gradients<br>‚Ä¢ Solutions: ReLU, residual connections, gradient clipping |
| **C.17 Softmax & Cross-Entropy Gradient** | 2026-02-03 | High | ‚Ä¢ Softmax: ≈∑·µ¢ = e^z·µ¢ / Œ£e^z‚±º<br>‚Ä¢ Cross-entropy: L = -Œ£ y·µ¢ log(≈∑·µ¢)<br>‚Ä¢ ‚àÇ≈∑·µ¢/‚àÇz·µ¢ = ≈∑·µ¢(1-≈∑·µ¢), ‚àÇ≈∑·µ¢/‚àÇz‚±º = -≈∑·µ¢¬∑≈∑‚±º (i‚â†j)<br>‚Ä¢ Final gradient: ‚àÇL/‚àÇz‚±º = ≈∑‚±º - y‚±º (same as binary!)<br>‚Ä¢ One-hot vector sums to 1 ‚Üí enables simplification<br>‚Ä¢ Practical benefits: numerical stability, simple implementation |
| **C.18 BatchNorm vs LayerNorm** *(in progress)* | 2026-02-09 | Medium | ‚Ä¢ BatchNorm: across batch dim; LayerNorm: across feature dim<br>‚Ä¢ LayerNorm for NLP: no batch dependency, same at train/test<br>‚Ä¢ Padding pollution issue with BatchNorm on variable-length sequences<br>‚Ä¢ RMSNorm: removes mean centering + beta, ~10-15% faster<br>‚Ä¢ ‚ö†Ô∏è Review needed: BatchNorm placement (before activation), RMSNorm details |

### D. NLP
| Topic | Date Mastered | Confidence | Key Points |
|-------|---------------|------------|------------|
| ‚Äî | ‚Äî | ‚Äî | ‚Äî |

### E. ML System Design
| Topic | Date Mastered | Confidence | Key Points |
|-------|---------------|------------|------------|
| **E.30 End-to-End ML Pipeline Design** | 2026-02-03 | Medium-High | ‚Ä¢ Start with business metrics, not models<br>‚Ä¢ Pipeline: Requirements ‚Üí Data ‚Üí Features ‚Üí Model ‚Üí Serving ‚Üí Evaluation<br>‚Ä¢ Baseline first (heuristics/logistic regression), iterate to complexity<br>‚Ä¢ Always frame improvements relative to current system |
| **E.31 Feature Engineering & Feature Stores** | 2026-02-03 | Medium-High | ‚Ä¢ Offline (Spark/Hive, batch) vs Online (Flink/Redis, streaming)<br>‚Ä¢ Training-serving skew: same feature computed differently<br>‚Ä¢ Solutions: log-and-wait, unified computation, feature validation<br>‚Ä¢ Some features fundamentally different: percentiles, global aggs, joins, ranks<br>‚Ä¢ Hybrid: slow-changing offline, fast-changing online |
| **E.35 A/B Testing & Experimentation** | 2026-02-03 | Medium-High | ‚Ä¢ ML tests harder: delayed feedback, smaller effects, feedback loops<br>‚Ä¢ Novelty effects, position bias<br>‚Ä¢ Filter bubble: only learn about what you show<br>‚Ä¢ Solutions: exploration (epsilon-greedy, Thompson sampling), IPW<br>‚Ä¢ Feature leakage: temporal availability at prediction time |
| **E.36 Monitoring & Model Degradation** | 2026-02-09 | Medium-High | ‚Ä¢ Three drift types: covariate, label, concept<br>‚Ä¢ Label shift = same pattern, different rate; Concept drift = relationship changes<br>‚Ä¢ 4-layer monitoring: data, model, operational, business<br>‚Ä¢ Operational monitoring segmented by pipeline step<br>‚Ä¢ Alert tiering: P0 (immediate), P1 (hours), P2 (daily)<br>‚Ä¢ Prevention: scheduled retraining, online learning, human-in-the-loop |

### F. Practical ML
| Topic | Date Mastered | Confidence | Key Points |
|-------|---------------|------------|------------|
| ‚Äî | ‚Äî | ‚Äî | ‚Äî |

---

## Knowledge Gaps

### üî¥ High Priority (Must fix before interview)
- None identified yet

### üü° Medium Priority (Should review)
- Full end-to-end system design practice (need structured practice)
- BatchNorm placement details (before vs after activation ‚Äî original paper says before)
- RMSNorm precise mechanics (removes mean centering + beta, not variance)

### üü¢ Recently Resolved
| Gap | Resolution Date | Notes |
|-----|-----------------|-------|
| Chain rule application in multi-layer networks | 2026-02-03 | Covered in backprop derivation |
| Softmax derivative mechanics | 2026-02-03 | Derived both cases (i=j, i‚â†j) |
| Monitoring & model degradation in production | 2026-02-09 | Built 4-layer monitoring framework, mastered drift types |

---

## Interview Readiness Checklist

### Can You Confidently...

**Fundamentals**
- [x] Derive gradient descent update rules
- [ ] Explain bias-variance tradeoff with examples
- [x] Compare L1 vs L2 regularization (L2 covered in logistic regression)
- [x] Explain AUC-ROC and when to use vs Precision@k
- [ ] Calculate precision, recall, F1 from confusion matrix

**Classical ML**
- [ ] Explain how random forests reduce variance
- [x] Derive logistic regression gradient (with L2 regularization!)
- [ ] Explain SVM margin and kernel trick
- [ ] Describe K-means algorithm and limitations

**Deep Learning**
- [x] Walk through backpropagation step by step
- [x] Explain vanishing gradients and solutions
- [x] Describe attention mechanism and transformers in detail
- [x] Compare BERT vs GPT architectures and use cases
- [x] Explain multi-head attention and W^O projection
- [x] Derive softmax + cross-entropy gradient
- [ ] Compare batch norm vs layer norm (in progress ‚Äî review details)

**NLP**
- [ ] Explain Word2Vec (skip-gram and CBOW)
- [x] Describe transformer architecture
- [ ] Explain BERT pre-training objectives

**System Design**
- [x] Design an end-to-end ML pipeline
- [x] Explain A/B testing for ML models
- [x] Discuss feature store architecture and tradeoffs
- [x] Identify and prevent feature leakage
- [x] Handle data drift scenarios (covariate, label, concept drift)
- [x] Design a monitoring framework (4-layer: data, model, operational, business)
- [ ] Discuss model serving trade-offs

---

## Study Plan

### This Week's Focus
1. [x] A.8 AUC-ROC and evaluation metrics (COMPLETED)
2. [x] A.3 Logistic regression gradient derivation with L2 regularization (COMPLETED)
3. [x] C.21 Transformers & self-attention mechanism (COMPLETED)
4. [x] Multi-head attention - why multiple heads? (COMPLETED)
5. [x] C.16 Backpropagation for simple neural network (COMPLETED)
6. [x] Softmax & cross-entropy gradient (multi-class extension) (COMPLETED)
7. [x] E.30 End-to-end ML pipeline design (COMPLETED)

### Upcoming Topics
- [x] E.36 Monitoring and model degradation (COMPLETED 2026-02-09)
- [ ] Batch norm vs Layer norm (IN PROGRESS ‚Äî review details needed)
- [ ] C.19 CNNs (convolutions, pooling, architectures)
- [ ] C.20 RNNs, LSTMs, GRUs (vanishing gradients, gating)
- [ ] A.5 Bias-variance tradeoff
- [ ] B.10 Decision trees, random forests, gradient boosting

### Review Scheduled
- [ ] Multi-head attention (reinforce W^O understanding)
- [ ] Backprop derivation (practice on whiteboard)

---

## Session History Summary

| Date | Topics Covered | Key Wins | Gaps Found |
|------|---------------|----------|------------|
| 2026-02-02 (Session 1) | A.8 AUC-ROC evaluation metric | ‚Ä¢ Understood ROC curve (corrected axis confusion)<br>‚Ä¢ Mastered AUC vs Precision@k tradeoffs<br>‚Ä¢ Can apply to real scenarios (rec sys, fraud detection)<br>‚Ä¢ Interview-ready for AUC questions | ‚Ä¢ Initially confused ROC axes (resolved)<br>‚Ä¢ Needed clarification on metric selection (resolved) |
| 2026-02-02 (Session 2) | A.3 Logistic regression gradient w/ L2 regularization | ‚Ä¢ **First use of 3-step structured workflow - success!**<br>‚Ä¢ Derived complete gradient from first principles<br>‚Ä¢ Mastered chain rule application in ML<br>‚Ä¢ Understood beautiful simplification: ‚àÇL/‚àÇz = ≈∑ - y<br>‚Ä¢ Can perform whiteboard derivation<br>‚Ä¢ Grasped weight decay intuition | ‚Ä¢ Chain rule was fuzzy (resolved with review)<br>‚Ä¢ Made errors on BCE derivative (corrected)<br>‚Ä¢ Minor: didn't cover bias gradient or batch averaging |
| 2026-02-02 (Session 3) | C.21 Transformers & self-attention mechanism | ‚Ä¢ **Student had exceptional baseline knowledge!**<br>‚Ä¢ Structured understanding into interview-ready format<br>‚Ä¢ Self-attention: Q, K, V mechanism and O(n¬≤) trade-off<br>‚Ä¢ Positional encodings: RoPE, sinusoidal, learned<br>‚Ä¢ BERT vs GPT: encoder/decoder, bidirectional/causal<br>‚Ä¢ Applied knowledge to practical scenarios<br>‚Ä¢ **3 topics mastered in one day!** | ‚Ä¢ Minor: didn't know about causal masking in GPT (added)<br>‚Ä¢ Minor: less familiar with all positional encoding types (covered) |
| 2026-02-03 (Session 4) | Multi-head attention, Backprop, Softmax+CE, ML Pipelines | ‚Ä¢ **4 major topics in one session!**<br>‚Ä¢ Strong math derivations for backprop and softmax<br>‚Ä¢ Connected concepts across sessions<br>‚Ä¢ ML System Design shows real-world experience<br>‚Ä¢ Can whiteboard multi-head attention and gradients | ‚Ä¢ Minor derivative mechanics (corrected in session)<br>‚Ä¢ Could use more system design practice |
| 2026-02-09 (Session 5) | E.36 Monitoring & Degradation, C.18 BatchNorm vs LayerNorm | ‚Ä¢ Built 4-layer monitoring framework (interview-ready)<br>‚Ä¢ Mastered drift types (covariate, label, concept)<br>‚Ä¢ Strong operational monitoring with segmentation<br>‚Ä¢ Understood BatchNorm vs LayerNorm trade-offs<br>‚Ä¢ Quiz: weighted cross-entropy derivation started | ‚Ä¢ Label shift vs concept drift initially unclear (resolved)<br>‚Ä¢ BatchNorm placement (corrected: before activation)<br>‚Ä¢ RMSNorm details slightly inaccurate (corrected) |

---

*This tracker is your single source of truth for interview preparation progress.*
